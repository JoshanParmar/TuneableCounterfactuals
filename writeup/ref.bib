@book{molnar_interpretable_2023,
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	author = {Molnar, Christoph},
	file = {Snapshot:/Users/joshanparmar/Zotero/storage/7AIS6Q7P/logistic.html:text/html},
	month = mar,
	publisher = {Independently published},
	title = {Interpretable {Machine} {Learning}},
	url = {https://christophm.github.io/interpretable-ml-book/logistic.html},
	urldate = {2023-05-12},
	year = {2023},
	bdsk-url-1 = {https://christophm.github.io/interpretable-ml-book/logistic.html}
}


@article{castelvecchi_can_2016,
	abstract = {Artificial intelligence is everywhere. But before scientists trust it, they first need to understand how machines learn.},
	annote = {Cg\_type: Nature News Section: News Feature},
	author = {Castelvecchi, Davide},
	doi = {10.1038/538020a},
	file = {Full Text:/Users/joshanparmar/Zotero/storage/R8NE8VZ3/Castelvecchi - 2016 - Can we open the black box of AI.pdf:application/pdf;Snapshot:/Users/joshanparmar/Zotero/storage/Y3LC6TRR/can-we-open-the-black-box-of-ai-1.html:text/html},
	journal = {Nature News},
	language = {en},
	month = oct,
	number = {7623},
	pages = {20},
	title = {Can we open the black box of {AI}?},
	url = {http://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731},
	urldate = {2023-05-03},
	volume = {538},
	year = {2016},
	bdsk-url-1 = {http://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731},
	bdsk-url-2 = {https://doi.org/10.1038/538020a}
}


@book{van_rossum_python_1995,
	author = {Van Rossum, Guido and Drake Jr, Fred L},
	date-added = {2023-12-01 12:07:51 +0000},
	date-modified = {2023-12-01 12:07:51 +0000},
	publisher = {Centrum voor Wiskunde en Informatica Amsterdam, The Netherlands},
	title = {Python tutorial},
	year = {1995}
}


@misc{klaise_alibi_2021,
	abstract = {Algorithms for explaining machine learning models},
	annote = {Issue: 181 Pages: 1--7 Publication Title: Journal of Machine Learning Research Volume: 22 original-date: 2019-02-26T10:10:56Z},
	author = {Klaise, Janis and Van Looveren, Arnaud and Vacanti, Giovanni and Coca, Alexandru},
	copyright = {Apache-2.0},
	date-added = {2023-12-01 12:08:28 +0000},
	date-modified = {2023-12-01 12:08:28 +0000},
	month = jun,
	shorttitle = {Alibi {Explain}},
	title = {Alibi {Explain}: {Algorithms} for {Explaining} {Machine} {Learning} {Models}},
	url = {http://jmlr.org/papers/v22/21-0017.html},
	urldate = {2023-04-27},
	year = {2021},
	bdsk-url-1 = {http://jmlr.org/papers/v22/21-0017.html}}


@misc{wachter_counterfactual_2017,
	abstract = {There has been much discussion of the ``right to explanation'' in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the `black box' of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Data controllers have an interest to not disclose information about their algorithms that contains trade secrets, violates the rights and freedoms of others (e.g. privacy), or allows data subjects to game or manipulate decision-making.Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support.From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR, and the extent to which they hinge on opening the `black box'. We suggest data controllers should offer a particular type of explanation, `unconditional counterfactual explanations', to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the ``closest possible world.'' As multiple variables or sets of variables can lead to one or more desirable outcomes, multiple counterfactual explanations can be provided, corresponding to different choices of nearby possible worlds for which the counterfactual holds. Counterfactuals describe a dependency on the external facts that lead to that decision without the need to convey the internal state or logic of an algorithm. As a result, counterfactuals serve as a minimal solution that bypasses the current technical limitations of interpretability, while striking a balance between transparency and the rights and freedoms of others (e.g. privacy, trade secrets).},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	date-added = {2023-12-01 12:09:26 +0000},
	date-modified = {2023-12-01 12:09:26 +0000},
	doi = {10.2139/ssrn.3063289},
	file = {Full Text PDF:/Users/joshanparmar/Zotero/storage/2BYH6DV7/Wachter et al. - 2017 - Counterfactual Explanations Without Opening the Bl.pdf:application/pdf},
	keywords = {machine learning, accountability, algorithms, artificial intelligence, auditing, automated decision-making, counterfactuals, data ethics, data protection, explainability, GDPR, interpretability, right to explanation, transparency},
	language = {en},
	month = oct,
	note = {Place: Rochester, NY Type: SSRN Scholarly Paper},
	shorttitle = {Counterfactual {Explanations} {Without} {Opening} the {Black} {Box}},
	title = {Counterfactual {Explanations} {Without} {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	url = {https://papers.ssrn.com/abstract=3063289},
	urldate = {2023-04-23},
	year = {2017},
	bdsk-url-1 = {https://papers.ssrn.com/abstract=3063289},
	bdsk-url-2 = {https://doi.org/10.2139/ssrn.3063289}}



@misc{kingma_adam_2017,
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	date-added = {2023-12-01 12:12:44 +0000},
	date-modified = {2023-12-01 12:12:44 +0000},
	doi = {10.48550/arXiv.1412.6980},
	file = {arXiv Fulltext PDF:/Users/joshanparmar/Zotero/storage/KT6VIVEY/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/joshanparmar/Zotero/storage/V7RF9BUC/1412.html:text/html},
	keywords = {Computer Science - Machine Learning},
	month = jan,
	publisher = {arXiv},
	shorttitle = {Adam},
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1412.6980},
	urldate = {2023-04-25},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1412.6980},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1412.6980}}


@misc{van_looveren_conditional_2021,
	abstract = {Counterfactual instances offer human-interpretable insight into the local behaviour of machine learning models. We propose a general framework to generate sparse, in-distribution counterfactual model explanations which match a desired target prediction with a conditional generative model, allowing batches of counterfactual instances to be generated with a single forward pass. The method is flexible with respect to the type of generative model used as well as the task of the underlying predictive model. This allows straightforward application of the framework to different modalities such as images, time series or tabular data as well as generative model paradigms such as GANs or autoencoders and predictive tasks like classification or regression. We illustrate the effectiveness of our method on image (CelebA), time series (ECG) and mixed-type tabular (Adult Census) data.},
	annote = {Comment: 12 pages},
	author = {Van Looveren, Arnaud and Klaise, Janis and Vacanti, Giovanni and Cobb, Oliver},
	date-added = {2023-12-01 12:13:37 +0000},
	date-modified = {2023-12-01 12:13:37 +0000},
	file = {arXiv.org Snapshot:/Users/joshanparmar/Zotero/storage/4MHHRMA6/2101.html:text/html;Full Text PDF:/Users/joshanparmar/Zotero/storage/JHNM9F5A/Van Looveren et al. - 2021 - Conditional Generative Models for Counterfactual E.pdf:application/pdf},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	month = jan,
	publisher = {arXiv},
	title = {Conditional {Generative} {Models} for {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2101.10123},
	urldate = {2023-04-24},
	year = {2021},
	bdsk-url-1 = {http://arxiv.org/abs/2101.10123}}


@misc{jordan_introduction_2018,
	abstract = {Autoencoders are an unsupervised learning technique in which we leverage neural networks for the task of representation learning. Specifically, we'll design a neural network architecture such that we impose a bottleneck in the network which forces a compressed knowledge representation of the original input. If the input features were each},
	author = {Jordan, Jeremy},
	date-added = {2023-12-01 12:14:51 +0000},
	date-modified = {2023-12-01 12:14:51 +0000},
	file = {Snapshot:/Users/joshanparmar/Zotero/storage/MQRQZXHG/autoencoders.html:text/html},
	language = {en},
	month = mar,
	title = {Introduction to autoencoders.},
	url = {https://www.jeremyjordan.me/autoencoders/},
	urldate = {2023-05-14},
	year = {2018},
	bdsk-url-1 = {https://www.jeremyjordan.me/autoencoders/}}

@article{bentley_multidimensional_1975,
	abstract = {This paper develops the multidimensional binary search tree (or k -d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k -d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O (log n ); deletion of the root, O ( n ( k -1)/ k ); deletion of a random node, O (log n ); and optimization (guarantees logarithmic performance of searches), O ( n log n ). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O ( n ( k - t )/ k )] and for nearest neighbor queries [empirically observed average running time of O (log n ).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k -d trees could be quite useful in many applications, and examples of potential uses are given.},
	author = {Bentley, Jon Louis},
	date-added = {2023-12-01 12:16:38 +0000},
	date-modified = {2023-12-01 12:16:38 +0000},
	doi = {10.1145/361002.361007},
	file = {Full Text PDF:/Users/joshanparmar/Zotero/storage/6Z8B2JYM/Bentley - 1975 - Multidimensional binary search trees used for asso.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	language = {en},
	month = sep,
	number = {9},
	pages = {509--517},
	title = {Multidimensional binary search trees used for associative searching},
	url = {https://dl.acm.org/doi/10.1145/361002.361007},
	urldate = {2023-05-02},
	volume = {18},
	year = {1975},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/361002.361007},
	bdsk-url-2 = {https://doi.org/10.1145/361002.361007}}

@misc{inam_explainable_2021,
	abstract = {Understand how Explainable AI (XAI) increases explainability and trustworthiness of AI-based solutions.},
	author = {Inam, Rafia and Terra, Ahmad and Fersman, Elena and Feljan, Aneta Vulgarakis},
	date-added = {2023-12-01 12:17:11 +0000},
	date-modified = {2023-12-01 12:17:11 +0000},
	file = {Snapshot:/Users/joshanparmar/Zotero/storage/V938WR3C/explainable-ai--how-humans-can-trust-ai.html:text/html},
	language = {en},
	month = apr,
	shorttitle = {Explainable {AI}},
	title = {Explainable {AI}: {How} humans can trust {Artificial} {Intelligence}},
	url = {https://www.ericsson.com/en/reports-and-papers/white-papers/explainable-ai--how-humans-can-trust-ai},
	urldate = {2023-05-12},
	year = {2021},
	bdsk-url-1 = {https://www.ericsson.com/en/reports-and-papers/white-papers/explainable-ai--how-humans-can-trust-ai}}

@misc{blackwell_bias_2022,
	author = {Blackwell, Alan},
	date-added = {2023-12-01 12:17:42 +0000},
	date-modified = {2023-12-01 12:17:42 +0000},
	month = nov,
	note = {Place: University of Cambridge},
	title = {Bias and {Fairness}},
	url = {https://www.cl.cam.ac.uk/teaching/2223/IML/IWML-2022-fairness-bias.pdf},
	urldate = {2023-03-15},
	year = {2022},
	bdsk-url-1 = {https://www.cl.cam.ac.uk/teaching/2223/IML/IWML-2022-fairness-bias.pdf}}

@article{goodman_european_2017,
	author = {Goodman, Bryce and Flaxman, Seth},
	date-added = {2023-12-01 12:18:11 +0000},
	date-modified = {2023-12-01 12:18:11 +0000},
	doi = {10.1609/aimag.v38i3.2741},
	journal = {AI Magazine},
	month = oct,
	note = {Publisher: Wiley},
	number = {3},
	pages = {50--57},
	title = {European {Union} {Regulations} on {Algorithmic} {Decision}-{Making} and a ``{Right} to {Explanation}''},
	url = {https://doi.org/10.1609\%2Faimag.v38i3.2741},
	volume = {38},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1609%5C%2Faimag.v38i3.2741},
	bdsk-url-2 = {https://doi.org/10.1609/aimag.v38i3.2741}}

@techreport{office_of_ron_wyden_algorithmic_2022,
	author = {{Office of Ron Wyden}},
	date-added = {2023-12-01 12:18:52 +0000},
	date-modified = {2023-12-01 12:18:52 +0000},
	institution = {Office of Ron Wyden},
	month = feb,
	title = {Algorithmic {Accountability} {Act} of 2022 {One}-{Pager}},
	year = {2022}}

@misc{schut_generating_2021,
	abstract = {Counterfactual explanations (CEs) are a practical tool for demonstrating why machine learning classifiers make particular decisions. For CEs to be useful, it is important that they are easy for users to interpret. Existing methods for generating interpretable CEs rely on auxiliary generative models, which may not be suitable for complex datasets, and incur engineering overhead. We introduce a simple and fast method for generating interpretable CEs in a white-box setting without an auxiliary model, by using the predictive uncertainty of the classifier. Our experiments show that our proposed algorithm generates more interpretable CEs, according to IM1 scores, than existing methods. Additionally, our approach allows us to estimate the uncertainty of a CE, which may be important in safety-critical applications, such as those in the medical domain.},
	annote = {Comment: 21 pages, 13 Figures},
	author = {Schut, Lisa and Key, Oscar and McGrath, Rory and Costabello, Luca and Sacaleanu, Bogdan and Corcoran, Medb and Gal, Yarin},
	date-added = {2023-12-01 12:22:13 +0000},
	date-modified = {2023-12-01 12:22:13 +0000},
	file = {arXiv.org Snapshot:/Users/joshanparmar/Zotero/storage/NH5JCSJN/2103.html:text/html;Full Text PDF:/Users/joshanparmar/Zotero/storage/QJE5SQ3G/Schut et al. - 2021 - Generating Interpretable Counterfactual Explanatio.pdf:application/pdf},
	keywords = {Computer Science - Machine Learning, Statistics - Applications},
	month = mar,
	publisher = {arXiv},
	title = {Generating {Interpretable} {Counterfactual} {Explanations} {By} {Implicit} {Minimisation} of {Epistemic} and {Aleatoric} {Uncertainties}},
	url = {http://arxiv.org/abs/2103.08951},
	urldate = {2023-04-24},
	year = {2021},
	bdsk-url-1 = {http://arxiv.org/abs/2103.08951}}

@misc{saeed_mimic-ii_2011,
	abstract = {MIMIC-II documents a diverse and large population of intensive care unit patient stays and contains comprehensive and detailed clinical data, including physiological waveforms and minute-by-minute trends for a subset of records. It establishes a unique public-access resource for critical care research, supporting a diverse range of analytic studies spanning epidemiology, clinical decision-rule development, and electronic tool development. The MIMIC-II Clinical Database, although de-identified, still contains detailed information regarding the clinical care of patients, and must be treated with appropriate care and respect.},
	author = {Saeed, Mohammed and Villarroel, Mauricio and Reisner, Andrew and Clifford, Gari and Lehman, Li-wei and Moody, George and Heldt, Thomas and Kyaw, Tin and Moody, Benjamin and Mark, Roger},
	date-added = {2023-12-01 12:23:19 +0000},
	date-modified = {2023-12-01 12:23:19 +0000},
	doi = {10.13026/FXN0-MK84},
	publisher = {PhysioNet},
	title = {{MIMIC}-{II} {Clinical} {Database}},
	url = {https://physionet.org/content/mimic-ii/2.6.0/},
	urldate = {2023-05-18},
	year = {2011},
	bdsk-url-1 = {https://physionet.org/content/mimic-ii/2.6.0/},
	bdsk-url-2 = {https://doi.org/10.13026/FXN0-MK84}}

@misc{raffa_clinical_2016,
	abstract = {This dataset was created for the purpose of a case study in the book: [ \_Secondary Analysis of Electronic Health Records\_](https://link.springer.com/book/10.1007/978-3-319-43742-2) , published by Springer in 2016. In particular, the dataset was used throughout [Chapter 16 (Data Analysis) ](https://link.springer.com/chapter/10.1007/978-3-319-43742-2\_16)by Raffa J. et al. to investigate the effectiveness of indwelling arterial catheters in hemodynamically stable patients with respiratory failure for mortality outcomes. The dataset is derived from MIMIC-II, the publicly-accessible critical care database. It contains summary clinical data and outcomes for 1,776 patients.},
	author = {Raffa, Jesse},
	date-added = {2023-12-01 12:24:16 +0000},
	date-modified = {2023-12-01 12:24:16 +0000},
	doi = {10.13026/C2NC7F},
	publisher = {PhysioNet},
	shorttitle = {Clinical data from the {MIMIC}-{II} database for a case study on indwelling arterial catheters},
	title = {Clinical data from the {MIMIC}-{II} database for a case study on indwelling arterial catheters:},
	url = {https://physionet.org/content/mimic2-iaccd/1.0/},
	urldate = {2023-05-18},
	year = {2016},
	bdsk-url-1 = {https://physionet.org/content/mimic2-iaccd/1.0/},
	bdsk-url-2 = {https://doi.org/10.13026/C2NC7F}}

@misc{verma_counterfactual_2022,
	abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
	annote = {Comment: 23 pages (8 pages of references)},
	author = {Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan E. and Dickerson, John P. and Shah, Chirag},
	date-added = {2023-12-01 12:25:08 +0000},
	date-modified = {2023-12-01 12:25:08 +0000},
	file = {arXiv Fulltext PDF:/Users/joshanparmar/Zotero/storage/AP4KMXSZ/Verma et al. - 2022 - Counterfactual Explanations and Algorithmic Recour.pdf:application/pdf;arXiv.org Snapshot:/Users/joshanparmar/Zotero/storage/9Z3DBVVR/2010.html:text/html},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	month = nov,
	publisher = {arXiv},
	shorttitle = {Counterfactual {Explanations} and {Algorithmic} {Recourses} for {Machine} {Learning}},
	title = {Counterfactual {Explanations} and {Algorithmic} {Recourses} for {Machine} {Learning}: {A} {Review}},
	url = {http://arxiv.org/abs/2010.10596},
	urldate = {2023-04-20},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2010.10596}}

